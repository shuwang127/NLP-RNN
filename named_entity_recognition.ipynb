{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "named_entity_recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/shuwang127/NLP-RNN/blob/master/named_entity_recognition.ipynb",
      "authorship_tag": "ABX9TyP0ZfmBDBYc1X+5OQ3iIJPk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuwang127/NLP-RNN/blob/master/named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Vh77DMB03z",
        "colab_type": "code",
        "outputId": "a8c667d6-1d5c-4e7e-b204-a95db8216af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "import sys\n",
        "import re\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from gensim import models\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torchdata\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# global path\n",
        "rootPath = './drive/My Drive/Colab Notebooks/'\n",
        "dataPath = rootPath + '/conll2003/'\n",
        "tempPath = rootPath + '/temp/'\n",
        "# global variable\n",
        "numEpoch = 100\n",
        "perEpoch = 5\n",
        "learnRate = 0.0001\n",
        "\n",
        "# Logger: redirect the stream on screen and to file.\n",
        "class Logger(object):\n",
        "    def __init__(self, filename = \"log.txt\"):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, \"a\")\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "    def flush(self):\n",
        "        pass\n",
        "\n",
        "def main():\n",
        "    # initialize the log file.\n",
        "    sys.stdout = Logger()\n",
        "    print(\"-- AIT726 Homework 3 from Julia Jeng, Shu Wang, and Arman Anwar --\")\n",
        "    # read data from files.\n",
        "    dataTrain, dataValid, dataTest, vocab = ReadData()\n",
        "    # get mapping.\n",
        "    wordDict, classDict = GetDict(vocab)\n",
        "    dTrain, lTrain = GetMapping(dataTrain, wordDict, classDict)\n",
        "    dValid, lValid = GetMapping(dataValid, wordDict, classDict)\n",
        "    dTest, lTest = GetMapping(dataTest, wordDict, classDict)\n",
        "    # get the embedding.\n",
        "    preWeights = GetEmbedding(wordDict)\n",
        "    # demo\n",
        "    #DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=True, Type='RNN', bidirect=False)\n",
        "    #DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=True, Type='RNN', bidirect=True)\n",
        "    #DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=True, Type='LSTM', bidirect=False)\n",
        "    #DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=True, Type='LSTM', bidirect=True)\n",
        "    #DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=True, Type='GRU', bidirect=False)\n",
        "    #DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=True, Type='GRU', bidirect=True)\n",
        "    DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=False, Type='LSTM', bidirect=True)\n",
        "    return\n",
        "\n",
        "def ReadData():\n",
        "    # lower case capitalized words.\n",
        "    def LowerCase(data):\n",
        "        def LowerFunc(matched):\n",
        "            return matched.group(1).lower()\n",
        "        pattern = r'([A-Z]+[a-z]+)'\n",
        "        data = re.sub(pattern, LowerFunc, data)\n",
        "        return data\n",
        "\n",
        "    vocab = defaultdict(list)\n",
        "    dataTrain = []\n",
        "    sentence = []\n",
        "    # read text from train file.\n",
        "    file = open(dataPath + '/train.txt').read()\n",
        "    for word in file.splitlines():\n",
        "        seg = word.split(' ')\n",
        "        if len(seg) <= 1:\n",
        "            if (sentence):\n",
        "                dataTrain.append(sentence)\n",
        "            sentence = []\n",
        "        else:\n",
        "            if seg[0] != '-DOCSTART-':\n",
        "                sentence.append([LowerCase(seg[0]), seg[-1]])\n",
        "                vocab[LowerCase(seg[0])].append(seg[-1])\n",
        "    # get the max length of dataTrain.\n",
        "    maxlenTrain = max([len(sentence) for sentence in dataTrain])\n",
        "    print('[Info] Load %d training sentences (max:%d words) from %s/train.txt.' % (len(dataTrain), maxlenTrain, dataPath))\n",
        "\n",
        "    dataValid = []\n",
        "    sentence = []\n",
        "    # read text from valid file.\n",
        "    file = open(dataPath + '/valid.txt').read()\n",
        "    for word in file.splitlines():\n",
        "        seg = word.split(' ')\n",
        "        if len(seg) <= 1:\n",
        "            if (sentence):\n",
        "                dataValid.append(sentence)\n",
        "            sentence = []\n",
        "        else:\n",
        "            if seg[0] != '-DOCSTART-':\n",
        "                sentence.append([LowerCase(seg[0]), seg[-1]])\n",
        "                vocab[LowerCase(seg[0])].append(seg[-1])\n",
        "    # get the max length of dataValid.\n",
        "    maxlenValid = max([len(sentence) for sentence in dataValid])\n",
        "    print('[Info] Load %d validation sentences (max:%d words) from %s/valid.txt.' % (len(dataValid), maxlenValid, dataPath))\n",
        "\n",
        "    dataTest = []\n",
        "    sentence = []\n",
        "    # read text from test file.\n",
        "    file = open(dataPath + '/test.txt').read()\n",
        "    for word in file.splitlines():\n",
        "        seg = word.split(' ')\n",
        "        if len(seg) <= 1:\n",
        "            if (sentence):\n",
        "                dataTest.append(sentence)\n",
        "            sentence = []\n",
        "        else:\n",
        "            if seg[0] != '-DOCSTART-':\n",
        "                sentence.append([LowerCase(seg[0]), seg[-1]])\n",
        "                vocab[LowerCase(seg[0])].append(seg[-1])\n",
        "    # get the max length of dataTest.\n",
        "    maxlenTest = max([len(sentence) for sentence in dataTest])\n",
        "    print('[Info] Load %d testing sentences (max:%d words) from %s/test.txt.' % (len(dataTest), maxlenTest, dataPath))\n",
        "\n",
        "    # the max length of dataTrain, dataValid and dataTest.\n",
        "    maxlen = max(maxlenTrain, maxlenValid, maxlenTest)\n",
        "    # append 0s at the end of sentence to the max length.\n",
        "    for sentence in dataTrain:\n",
        "        sentence.extend(['0', '<pad>'] for i in range(maxlen - len(sentence)))\n",
        "    for sentence in dataValid:\n",
        "        sentence.extend(['0', '<pad>'] for i in range(maxlen - len(sentence)))\n",
        "    for sentence in dataTest:\n",
        "        sentence.extend(['0', '<pad>'] for i in range(maxlen - len(sentence)))\n",
        "\n",
        "    # clear up the vocabulary.\n",
        "    for word, label in vocab.items():\n",
        "        vocab[word] = list(set(label))\n",
        "    vocab['0'] = '<pad>'\n",
        "    print('[Info] Get %d vocabulary words successfully.' % (len(vocab)))\n",
        "\n",
        "    return dataTrain, dataValid, dataTest, vocab\n",
        "\n",
        "def GetDict(vocab):\n",
        "    # get word and class dictionary.\n",
        "    wordDict = {word: index for index, word in enumerate(vocab)}\n",
        "    classDict = {'<pad>': 0, 'O': 1, 'B-ORG': 2, 'B-PER': 3, 'B-LOC': 4, 'B-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'I-LOC': 8, 'I-MISC': 9}\n",
        "    # return\n",
        "    return wordDict, classDict\n",
        "\n",
        "def GetMapping(data, wordDict, classDict):\n",
        "    # map data and label to index-form.\n",
        "    data2index = []\n",
        "    label2index = []\n",
        "    for sentence in data:\n",
        "        # for each sentence.\n",
        "        sent2index = []\n",
        "        lb2index = []\n",
        "        for word in sentence:\n",
        "            sent2index.append(wordDict[word[0]])\n",
        "            lb2index.append(classDict[word[1]])\n",
        "        data2index.append(sent2index)\n",
        "        label2index.append(lb2index)\n",
        "    # return\n",
        "    return np.array(data2index), np.array(label2index)\n",
        "\n",
        "def GetEmbedding(wordDict):\n",
        "    # load preWeights.\n",
        "    weightFile = 'preWeights.npy'\n",
        "    if not os.path.exists(tempPath + '/' + weightFile):\n",
        "        # find embedding file.\n",
        "        embedFile = 'GoogleNews.txt'\n",
        "        if not os.path.exists(tempPath + '/' + embedFile):\n",
        "            # path validation.\n",
        "            modelFile = 'GoogleNews-vectors-negative300.bin'\n",
        "            if not os.path.exists(tempPath + '/' + modelFile):\n",
        "                print('[Error] Cannot find %s/%s.' % (tempPath, modelFile))\n",
        "                return\n",
        "            # find word2vec file.\n",
        "            model = models.KeyedVectors.load_word2vec_format(tempPath + '/' + modelFile, binary=True)\n",
        "            model.save_word2vec_format(tempPath + '/' + embedFile)\n",
        "            print('[Info] Get the word2vec format file %s/%s.' % (tempPath, embedFile))\n",
        "\n",
        "        # read embedding file.\n",
        "        embedVec = {}\n",
        "        file = open(tempPath + '/' + embedFile, encoding = 'utf8')\n",
        "        for line in file:\n",
        "            seg = line.split()\n",
        "            word = seg[0]\n",
        "            embed = np.asarray(seg[1:], dtype = 'float32')\n",
        "            embedVec[word] = embed\n",
        "        np.save(tempPath+'/embedVec.npy', embedVec)\n",
        "\n",
        "        # get mapping to preWeights.\n",
        "        numWords = len(wordDict)\n",
        "        numDims = 300\n",
        "        preWeights = np.zeros((numWords, numDims))\n",
        "        for ind, word in enumerate(wordDict):\n",
        "            if word in embedVec:\n",
        "                preWeights[ind] = embedVec[word]\n",
        "            else:\n",
        "                preWeights[ind] = np.random.normal(size=(numDims,))\n",
        "\n",
        "        # save the preWeights.\n",
        "        np.save(tempPath + '/' + weightFile, preWeights)\n",
        "        print('[Info] Get pre-trained word2vec weights.')\n",
        "    else:\n",
        "        preWeights = np.load(tempPath + '/' + weightFile)\n",
        "        print('[Info] Load pre-trained word2vec weights from %s/%s.' % (tempPath, weightFile))\n",
        "    return preWeights\n",
        "\n",
        "class RecurrentNeuralNetwork(nn.Module):\n",
        "    def __init__(self, preWeights, preTrain=True, Type='RNN', bidirect=False, hiddenSize=256):\n",
        "        super(RecurrentNeuralNetwork, self).__init__()\n",
        "        # sparse parameters.\n",
        "        numWords, numDims = preWeights.size()\n",
        "        numBiDirect = 2 if bidirect else 1\n",
        "        # embedding layer.\n",
        "        self.embedding = nn.Embedding(num_embeddings=numWords, embedding_dim=numDims)\n",
        "        self.embedding.load_state_dict({'weight': preWeights})\n",
        "        if preTrain:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        # RNN layer.\n",
        "        if 'RNN' == Type:\n",
        "            self.rnn = nn.RNN(input_size=numDims, hidden_size=hiddenSize, batch_first=True, bidirectional=bidirect)\n",
        "        elif 'LSTM' == Type:\n",
        "            self.rnn = nn.LSTM(input_size=numDims, hidden_size=hiddenSize, batch_first=True, bidirectional=bidirect)\n",
        "        elif 'GRU' == Type:\n",
        "            self.rnn = nn.GRU(input_size=numDims, hidden_size=hiddenSize, batch_first=True, bidirectional=bidirect)\n",
        "        else:\n",
        "            print('[Error] RNN Type \\'%s\\' invalid!' % (Type))\n",
        "        # fully-connected layer.\n",
        "        self.fc = nn.Linear(in_features=hiddenSize*numBiDirect, out_features=10)\n",
        "        self.sm = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        out, _ = self.rnn(embeds)\n",
        "        out = out.contiguous().view(-1, out.shape[2])\n",
        "        yhats = self.fc(out)\n",
        "        return yhats\n",
        "\n",
        "def DemoRNN(dTrain, lTrain, dValid, lValid, dTest, lTest, preWeights, preTrain=True, Type='RNN', bidirect=False, hiddenSize=256, batchsize=256):\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dValid).long().cuda()\n",
        "    yValid = torch.from_numpy(lValid).long().cuda()\n",
        "    # batch size processing.\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # get training weights\n",
        "    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n",
        "    weights = []\n",
        "    for lb in range(1, 10):\n",
        "        weights.append(1 - (lbTrain.count(lb) / (len(lbTrain) - lbTrain.count(0))))\n",
        "    weights.insert(0, 0)\n",
        "    lbWeights = torch.FloatTensor(weights).cuda()\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWeights = torch.from_numpy(preWeights)\n",
        "    model = RecurrentNeuralNetwork(preWeights, preTrain=preTrain, Type=Type, bidirect=bidirect, hiddenSize=hiddenSize)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print('[Demo] --- RNNType: %s | HiddenNodes: %d | Bi-Direction: %s | Pre-Trained: %s ---' % (Type, hiddenSize, bidirect, preTrain))\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(numEpoch):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        lossTrain /= len(lbTrain)\n",
        "        # train accuracy.\n",
        "        padIndex = [ind for ind, lb in enumerate(labels) if lb == 0]\n",
        "        for ind in sorted(padIndex, reverse=True):\n",
        "            del predictions[ind]\n",
        "            del labels[ind]\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "                torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # valid accuracy.\n",
        "        padIndex = [ind for ind, lb in enumerate(labels) if lb == 0]\n",
        "        for ind in sorted(padIndex, reverse=True):\n",
        "            del predictions[ind]\n",
        "            del labels[ind]\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        # output information.\n",
        "        if 0 == (epoch + 1) % perEpoch:\n",
        "            print('[Epoch %03d] loss: %.3f, train acc: %.3f%%, valid acc: %.3f%%' % (epoch + 1, lossTrain, accTrain, accValid))\n",
        "        # save the best model.\n",
        "        if accList[-1] > max(accList[0:-1]):\n",
        "            torch.save(model.state_dict(), tempPath + '/model.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch + 1) >= 5 and accList[-1] < min(accList[-5:-1]):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model.pth'))\n",
        "\n",
        "    # test period\n",
        "    xTest = torch.from_numpy(dTest).long().cuda()\n",
        "    yTest = torch.from_numpy(lTest).long().cuda()\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "    # testing phase.\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for iter, (data, label) in enumerate(testloader):\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            yhat = model.forward(data)  # get output\n",
        "            # statistic\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # testing accuracy.\n",
        "    padIndex = [ind for ind, lb in enumerate(labels) if lb == 0]\n",
        "    for ind in sorted(padIndex, reverse=True):\n",
        "        del predictions[ind]\n",
        "        del labels[ind]\n",
        "    accuracy = accuracy_score(labels, predictions) * 100\n",
        "    print('[Info] Testing accuracy: %.3f%%' % (accuracy))\n",
        "\n",
        "    '''\n",
        "    print(\"--- Formatting Results for conlleval.py Official Evaluation --- %s seconds ---\" % (round((time.time() - start_time),2)))\n",
        "    formattedcontexts = []\n",
        "    formattedlabels = []\n",
        "    formattedpredictions = []\n",
        "    for element in labelsfull: #convert to real words and labels\n",
        "        formattedlabels.extend(labelindex[element])\n",
        "    for element in predictionsfull:\n",
        "        if element == 0:\n",
        "            element = 1 #remove stray <pad> predictions\n",
        "        formattedpredictions.extend(labelindex[element])\n",
        "    for element in contextfull:\n",
        "        formattedcontexts.extend(wordindex[element])\n",
        "    #write to file\n",
        "    fname = 'results/{}--bidir={}--hidden_size={}--pretrain={}--results.txt'.format(RNNTYPE,bidirectional,hidden_dim,pretrained_embeddings_status)\n",
        "    if os.path.exists(fname):\n",
        "        os.remove(fname)\n",
        "    f = open(fname,'w')\n",
        "    for (i,element) in enumerate(labelsfull):\n",
        "        f.write(formattedcontexts[i] + ' ' + formattedlabels[i] + ' ' + formattedpredictions[i] + '\\n')\n",
        "    f.close()\n",
        "    print('--- {}--bidir={}--hidden_size={}--pretrain={}--results ---'.format(RNNTYPE,bidirectional,hidden_dim,pretrained_embeddings_status))\n",
        "    evaluate_conll_file(open(fname,'r')) #evaluate using conll script\n",
        "    '''\n",
        "    return model\n",
        "\n",
        "# The program entrance.\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- AIT726 Homework 3 from Julia Jeng, Shu Wang, and Arman Anwar --\n",
            "[Info] Load 14041 training sentences (max:113 words) from ./drive/My Drive/Colab Notebooks//conll2003//train.txt.\n",
            "[Info] Load 3250 validation sentences (max:109 words) from ./drive/My Drive/Colab Notebooks//conll2003//valid.txt.\n",
            "[Info] Load 3453 testing sentences (max:124 words) from ./drive/My Drive/Colab Notebooks//conll2003//test.txt.\n",
            "[Info] Get 28449 vocabulary words successfully.\n",
            "[Info] Load pre-trained word2vec weights from ./drive/My Drive/Colab Notebooks//temp//preWeights.npy.\n",
            "[Demo] --- RNNType: GRU | HiddenNodes: 256 | Bi-Direction: True | Pre-Trained: True ---\n",
            "[Epoch 005] loss: 0.533, train acc: 90.203%, valid acc: 88.945%\n",
            "[Epoch 010] loss: 0.250, train acc: 94.498%, valid acc: 92.456%\n",
            "[Epoch 015] loss: 0.117, train acc: 97.400%, valid acc: 92.899%\n",
            "[Epoch 020] loss: 0.065, train acc: 98.451%, valid acc: 92.594%\n",
            "[Info] Testing accuracy: 91.263%\n",
            "[Demo] --- RNNType: LSTM | HiddenNodes: 256 | Bi-Direction: True | Pre-Trained: False ---\n",
            "[Epoch 005] loss: 0.196, train acc: 96.291%, valid acc: 94.235%\n",
            "[Epoch 010] loss: 0.028, train acc: 99.431%, valid acc: 94.936%\n",
            "[Info] Testing accuracy: 92.639%\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}