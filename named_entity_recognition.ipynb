{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "named_entity_recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1vTLKG0UtZvCNc9DtZnxj9kxZzW6wIecG",
      "authorship_tag": "ABX9TyOO7w2ruNGE+cg4HDtgYZgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuwang127/NLP-RNN/blob/master/named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Vh77DMB03z",
        "colab_type": "code",
        "outputId": "c49f1075-f833-4a3f-b443-be462f0f585f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import sys\n",
        "import re\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from gensim import models\n",
        "import itertools\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torchdata\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "# global path\n",
        "rootPath = './drive/My Drive/Colab Notebooks/'\n",
        "dataPath = rootPath + '/conll2003/'\n",
        "tempPath = rootPath + '/temp/'\n",
        "\n",
        "# Logger: redirect the stream on screen and to file.\n",
        "class Logger(object):\n",
        "    def __init__(self, filename = \"log.txt\"):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, \"a\")\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "    def flush(self):\n",
        "        pass\n",
        "\n",
        "def main():\n",
        "    # initialize the log file.\n",
        "    sys.stdout = Logger()\n",
        "    print(\"-- AIT726 Homework 3 from Julia Jeng, Shu Wang, and Arman Anwar --\")\n",
        "    # read data from files.\n",
        "    dataTrain, dataValid, dataTest, vocab = ReadData()\n",
        "    # get mapping.\n",
        "    wordDict, classDict = GetDict(vocab)\n",
        "    dTrain, lTrain = GetMapping(dataTrain, wordDict, classDict)\n",
        "    dValid, lValid = GetMapping(dataValid, wordDict, classDict)\n",
        "    dTest, lTest = GetMapping(dataTest, wordDict, classDict)\n",
        "    # get the embedding.\n",
        "    preWeights = GetEmbedding(wordDict)\n",
        "\n",
        "    model = RNNTrain(dTrain, lTrain, dValid, lValid, preWeights, preTrain=True, Type='RNN', bidirect=False, hiddenSize=256)\n",
        "    RNNTest(model, dTest, lTest)\n",
        "    return\n",
        "\n",
        "def ReadData():\n",
        "    # lower case capitalized words.\n",
        "    def LowerCase(data):\n",
        "        def LowerFunc(matched):\n",
        "            return matched.group(1).lower()\n",
        "        pattern = r'([A-Z]+[a-z]+)'\n",
        "        data = re.sub(pattern, LowerFunc, data)\n",
        "        return data\n",
        "\n",
        "    vocab = defaultdict(list)\n",
        "    dataTrain = []\n",
        "    sentence = []\n",
        "    # read text from train file.\n",
        "    file = open(dataPath + '/train.txt').read()\n",
        "    for word in file.splitlines():\n",
        "        seg = word.split(' ')\n",
        "        if len(seg) <= 1:\n",
        "            if (sentence):\n",
        "                dataTrain.append(sentence)\n",
        "            sentence = []\n",
        "        else:\n",
        "            if seg[0] != '-DOCSTART-':\n",
        "                sentence.append([LowerCase(seg[0]), seg[-1]])\n",
        "                vocab[LowerCase(seg[0])].append(seg[-1])\n",
        "    # get the max length of dataTrain.\n",
        "    maxlenTrain = max([len(sentence) for sentence in dataTrain])\n",
        "    print('[Info] Load %d training sentences (max:%d words) from %s/train.txt.' % (len(dataTrain), maxlenTrain, dataPath))\n",
        "\n",
        "    dataValid = []\n",
        "    sentence = []\n",
        "    # read text from valid file.\n",
        "    file = open(dataPath + '/valid.txt').read()\n",
        "    for word in file.splitlines():\n",
        "        seg = word.split(' ')\n",
        "        if len(seg) <= 1:\n",
        "            if (sentence):\n",
        "                dataValid.append(sentence)\n",
        "            sentence = []\n",
        "        else:\n",
        "            if seg[0] != '-DOCSTART-':\n",
        "                sentence.append([LowerCase(seg[0]), seg[-1]])\n",
        "                vocab[LowerCase(seg[0])].append(seg[-1])\n",
        "    # get the max length of dataValid.\n",
        "    maxlenValid = max([len(sentence) for sentence in dataValid])\n",
        "    print('[Info] Load %d validation sentences (max:%d words) from %s/valid.txt.' % (len(dataValid), maxlenValid, dataPath))\n",
        "\n",
        "    dataTest = []\n",
        "    sentence = []\n",
        "    # read text from test file.\n",
        "    file = open(dataPath + '/test.txt').read()\n",
        "    for word in file.splitlines():\n",
        "        seg = word.split(' ')\n",
        "        if len(seg) <= 1:\n",
        "            if (sentence):\n",
        "                dataTest.append(sentence)\n",
        "            sentence = []\n",
        "        else:\n",
        "            if seg[0] != '-DOCSTART-':\n",
        "                sentence.append([LowerCase(seg[0]), seg[-1]])\n",
        "                vocab[LowerCase(seg[0])].append(seg[-1])\n",
        "    # get the max length of dataTest.\n",
        "    maxlenTest = max([len(sentence) for sentence in dataTest])\n",
        "    print('[Info] Load %d testing sentences (max:%d words) from %s/test.txt.' % (len(dataTest), maxlenTest, dataPath))\n",
        "\n",
        "    # the max length of dataTrain, dataValid and dataTest.\n",
        "    maxlen = max(maxlenTrain, maxlenValid, maxlenTest)\n",
        "    # append 0s at the end of sentence to the max length.\n",
        "    for sentence in dataTrain:\n",
        "        sentence.extend(['0', '<pad>'] for i in range(maxlen - len(sentence)))\n",
        "    for sentence in dataValid:\n",
        "        sentence.extend(['0', '<pad>'] for i in range(maxlen - len(sentence)))\n",
        "    for sentence in dataTest:\n",
        "        sentence.extend(['0', '<pad>'] for i in range(maxlen - len(sentence)))\n",
        "\n",
        "    # clear up the vocabulary.\n",
        "    for word, label in vocab.items():\n",
        "        vocab[word] = list(set(label))\n",
        "    vocab['0'] = '<pad>'\n",
        "    print('[Info] Get %d vocabulary words successfully.' % (len(vocab)))\n",
        "\n",
        "    return dataTrain, dataValid, dataTest, vocab\n",
        "\n",
        "def GetDict(vocab):\n",
        "    # get word and class dictionary.\n",
        "    wordDict = {word: index for index, word in enumerate(vocab)}\n",
        "    classDict = {'<pad>': 0, 'O': 1, 'B-ORG': 2, 'B-PER': 3, 'B-LOC': 4, 'B-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'I-LOC': 8, 'I-MISC': 9}\n",
        "    # return\n",
        "    return wordDict, classDict\n",
        "\n",
        "def GetMapping(data, wordDict, classDict):\n",
        "    # map data and label to index-form.\n",
        "    data2index = []\n",
        "    label2index = []\n",
        "    for sentence in data:\n",
        "        # for each sentence.\n",
        "        sent2index = []\n",
        "        lb2index = []\n",
        "        for word in sentence:\n",
        "            sent2index.append(wordDict[word[0]])\n",
        "            lb2index.append(classDict[word[1]])\n",
        "        data2index.append(sent2index)\n",
        "        label2index.append(lb2index)\n",
        "    # return\n",
        "    return np.array(data2index), np.array(label2index)\n",
        "\n",
        "def GetEmbedding(wordDict):\n",
        "    # load preWeights.\n",
        "    weightFile = 'preWeights.npy'\n",
        "    if not os.path.exists(tempPath + '/' + weightFile):\n",
        "        # find embedding file.\n",
        "        embedFile = 'GoogleNews.txt'\n",
        "        if not os.path.exists(tempPath + '/' + embedFile):\n",
        "            # path validation.\n",
        "            modelFile = 'GoogleNews-vectors-negative300.bin'\n",
        "            if not os.path.exists(tempPath + '/' + modelFile):\n",
        "                print('[Error] Cannot find %s/%s.' % (tempPath, modelFile))\n",
        "                return\n",
        "            # find word2vec file.\n",
        "            model = models.KeyedVectors.load_word2vec_format(tempPath + '/' + modelFile, binary=True)\n",
        "            model.save_word2vec_format(tempPath + '/' + embedFile)\n",
        "            print('[Info] Get the word2vec format file %s/%s.' % (tempPath, embedFile))\n",
        "\n",
        "        # read embedding file.\n",
        "        embedVec = {}\n",
        "        file = open(tempPath + '/' + embedFile, encoding = 'utf8')\n",
        "        for line in file:\n",
        "            seg = line.split()\n",
        "            word = seg[0]\n",
        "            embed = np.asarray(seg[1:], dtype = 'float32')\n",
        "            embedVec[word] = embed\n",
        "        np.save(tempPath+'/embedVec.npy', embedVec)\n",
        "\n",
        "        # get mapping to preWeights.\n",
        "        numWords = len(wordDict)\n",
        "        numDims = 300\n",
        "        preWeights = np.zeros((numWords, numDims))\n",
        "        for ind, word in enumerate(wordDict):\n",
        "            if word in embedVec:\n",
        "                preWeights[ind] = embedVec[word]\n",
        "            else:\n",
        "                preWeights[ind] = np.random.normal(size=(numDims,))\n",
        "\n",
        "        # save the preWeights.\n",
        "        np.save(tempPath + '/' + weightFile, preWeights)\n",
        "        print('[Info] Get pre-trained word2vec weights.')\n",
        "    else:\n",
        "        preWeights = np.load(tempPath + '/' + weightFile)\n",
        "        print('[Info] Load pre-trained word2vec weights from %s/%s.' % (tempPath, weightFile))\n",
        "    return preWeights\n",
        "\n",
        "class RecurrentNeuralNetwork(nn.Module):\n",
        "    def __init__(self, preWeights, preTrain=True, Type='RNN', bidirect=False, hiddenSize=256):\n",
        "        super(RecurrentNeuralNetwork, self).__init__()\n",
        "        # sparse parameters.\n",
        "        numWords, numDims = preWeights.size()\n",
        "        numBiDirect = 2 if bidirect else 1\n",
        "        # embedding layer.\n",
        "        self.embedding = nn.Embedding(num_embeddings=numWords, embedding_dim=numDims)\n",
        "        self.embedding.load_state_dict({'weight': preWeights})\n",
        "        if preTrain:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        # RNN layer.\n",
        "        if 'RNN' == Type:\n",
        "            self.rnn = nn.RNN(input_size=numDims, hidden_size=hiddenSize, batch_first=True, bidirectional=bidirect)\n",
        "        elif 'LSTM' == Type:\n",
        "            self.rnn = nn.LSTM(input_size=numDims, hidden_size=hiddenSize, batch_first=True, bidirectional=bidirect)\n",
        "        elif 'GRU' == Type:\n",
        "            self.rnn = nn.GRU(input_size=numDims, hidden_size=hiddenSize, batch_first=True, bidirectional=bidirect)\n",
        "        else:\n",
        "            print('[Error] RNN Type \\'%s\\' invalid!' % (Type))\n",
        "        # fully-connected layer.\n",
        "        self.fc = nn.Linear(in_features=hiddenSize*numBiDirect, out_features=10)\n",
        "        self.sm = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        out, _ = self.rnn(embeds)\n",
        "        out = out.contiguous().view(-1, out.shape[2])\n",
        "        yhats = self.fc(out)\n",
        "        return yhats\n",
        "\n",
        "def RNNTrain(dTrain, lTrain, dValid, lValid, preWeights, preTrain=True, Type='RNN', bidirect=False, hiddenSize=256):\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dValid).long().cuda()\n",
        "    yValid = torch.from_numpy(lValid).long().cuda()\n",
        "    # batch size processing.\n",
        "    batchsize = 256\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    numTrain = len(train)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    numValid = len(valid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # get training weights\n",
        "    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n",
        "    weights = []\n",
        "    for lb in range(1, 10):\n",
        "        weights.append(1 - (lbTrain.count(lb) / (len(lbTrain) - lbTrain.count(0))))\n",
        "    weights.insert(0, 0)\n",
        "    lbWeights = torch.FloatTensor(weights).cuda()\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWeights = torch.from_numpy(preWeights)\n",
        "    model = RecurrentNeuralNetwork(preWeights, preTrain=True, Type='RNN', bidirect=False, hiddenSize=256)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(100):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "        lossTrain /= len(lbTrain)\n",
        "        # train accuracy.\n",
        "        padIndex = [ind for ind, lb in enumerate(labels) if lb == 0]\n",
        "        for ind in sorted(padIndex, reverse=True):\n",
        "            del predictions[ind]\n",
        "            del labels[ind]\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "        # valid accuracy.\n",
        "        padIndex = [ind for ind, lb in enumerate(labels) if lb == 0]\n",
        "        for ind in sorted(padIndex, reverse=True):\n",
        "            del predictions[ind]\n",
        "            del labels[ind]\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        # output information.\n",
        "        if 0 == (epoch + 1) % 2:\n",
        "            print('[Epoch %03d] loss: %.3f, train acc: %.3f%%, valid acc: %.3f%%' % (epoch + 1, lossTrain, accTrain, accValid))\n",
        "        # save the best model.\n",
        "        if accList[-1] > max(accList[0:-1]):\n",
        "            torch.save(model.state_dict(), tempPath + '/model.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch + 1) >= 100 and accList[-1] < min(accList[-100:-1]):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model.pth'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def RNNTest(model, dTest, lTest):\n",
        "    # get predictions for testing samples with model parameters.\n",
        "    def GetPredictions(model, dTest):\n",
        "        xTest = torch.from_numpy(dTest).long().cuda()\n",
        "        yhat = model.forward(xTest)\n",
        "        preds = yhat.max(1)[1]\n",
        "        predictions = preds.int().tolist()\n",
        "        return predictions\n",
        "\n",
        "    # evaluate the predictions with gold labels, and get accuracy and confusion matrix.\n",
        "    def Evaluation(predictions, lTest):\n",
        "        # sparse the corresponding label.\n",
        "        labelTest = [item for sublist in lTest.tolist() for item in sublist]\n",
        "        D = len(labelTest)\n",
        "        cls = 10\n",
        "        # get confusion matrix.int\n",
        "        confusion = np.zeros((cls, cls), dtype=int)\n",
        "        for ind in range(D):\n",
        "            nRow = int(predictions[ind])\n",
        "            nCol = int(labelTest[ind])\n",
        "            confusion[nRow][nCol] += 1\n",
        "        # get accuracy.\n",
        "        padIndex = [ind for ind, lb in enumerate(labelTest) if lb == 0]\n",
        "        for ind in sorted(padIndex, reverse=True):\n",
        "            del predictions[ind]\n",
        "            del labelTest[ind]\n",
        "        accuracy = accuracy_score(labelTest, predictions) * 100\n",
        "        return accuracy, confusion\n",
        "\n",
        "    # get predictions for testing samples.\n",
        "    predictions = GetPredictions(model, dTest)\n",
        "    accuracy, confusion = Evaluation(predictions, lTest)\n",
        "    print('[Info] Testing accuracy: %.3f%%' % (accuracy))\n",
        "    return accuracy, confusion\n",
        "\n",
        "# The program entrance.\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- AIT726 Homework 3 from Julia Jeng, Shu Wang, and Arman Anwar --\n",
            "[Info] Load 14041 training sentences (max:113 words) from ./drive/My Drive/Colab Notebooks//conll2003//train.txt.\n",
            "[Info] Load 3250 validation sentences (max:109 words) from ./drive/My Drive/Colab Notebooks//conll2003//valid.txt.\n",
            "[Info] Load 3453 testing sentences (max:124 words) from ./drive/My Drive/Colab Notebooks//conll2003//test.txt.\n",
            "[Info] Get 28449 vocabulary words successfully.\n",
            "[Info] Load pre-trained word2vec weights from ./drive/My Drive/Colab Notebooks//temp//preWeights.npy.\n",
            "[Epoch 002] loss: 1.628, train acc: 82.371%, valid acc: 82.972%\n",
            "[Epoch 004] loss: 1.413, train acc: 83.078%, valid acc: 83.612%\n",
            "[Epoch 006] loss: 1.283, train acc: 83.548%, valid acc: 84.109%\n",
            "[Epoch 008] loss: 1.193, train acc: 83.868%, valid acc: 84.543%\n",
            "[Epoch 010] loss: 1.123, train acc: 84.477%, valid acc: 84.942%\n",
            "[Epoch 012] loss: 1.066, train acc: 84.883%, valid acc: 85.129%\n",
            "[Epoch 014] loss: 1.019, train acc: 85.109%, valid acc: 85.191%\n",
            "[Epoch 016] loss: 0.980, train acc: 85.370%, valid acc: 85.248%\n",
            "[Epoch 018] loss: 0.946, train acc: 85.499%, valid acc: 85.250%\n",
            "[Epoch 020] loss: 0.918, train acc: 85.667%, valid acc: 85.306%\n",
            "[Epoch 022] loss: 0.893, train acc: 85.766%, valid acc: 85.246%\n",
            "[Epoch 024] loss: 0.871, train acc: 85.896%, valid acc: 85.281%\n",
            "[Epoch 026] loss: 0.851, train acc: 86.058%, valid acc: 85.265%\n",
            "[Epoch 028] loss: 0.833, train acc: 86.153%, valid acc: 85.242%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}